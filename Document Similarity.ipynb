{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarun Sunkaraneni: Document Similarity Analysis\n",
    "\n",
    "### We will study how to define the distance between sets, specifically with the Jaccard distance. To illustrate and motivate this study, we will focus on using Jaccard distance to measure the distance between documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general approach for checking Document similarity is to _shingle_ the document (or create _k-grams_). This takes consecutive words and group them as a single object. A k-gram is a consecutive set of k words. So the set of all 1-grams is exactly the bag of words model. An alternative name to k-gram is a k-shingle; these mean the same thing.\n",
    ">D1: I am Sam.\n",
    "\n",
    ">D2: Sam I am.\n",
    "\n",
    ">D3 : I do not like green eggs and ham. \n",
    "\n",
    ">D4: I do not like them, Sam I am.\n",
    "\n",
    "The $(k=1)$-grams of $D1\\cup D2\\cup D3\\cup D4$ are: \n",
    "#### { [I], [am], [Sam], [do], [not], [like], [green], [eggs], [and], [ham], [them] }\n",
    "\n",
    "The $(k=2)$-grams of $D1\\cup D2\\cup D3 \\cup D4$ are: \n",
    "#### { [I am], [am Sam], [Sam Sam], [Sam I], [am I], [I do], [do not], [not like], [like green], [green eggs], [eggs and], [and ham], [like them], [them Sam] }\n",
    "The set of $k$-grams of a document with $n$ words is at most $n − k$. The takes space $O(kn)$ to store them all. If $k $ is small, this is not a high overhead. Furthermore, the space goes down as items are repeated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "# For the example here, we are assuming space as an unique character, \n",
    "# Also capital letters are different than lower cased letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. \"I am Sam\" = { ['I '], [' a'], ['am'], ['m '], [' S'], ['Sa'], ['am']}\n",
    "def twoCharGram(dataset):\n",
    "    with open(dataset, 'r') as data:\n",
    "        textFile = data.read().replace('\\n', ' ')\n",
    "        kGrams = set()\n",
    "        # 2-Char gram\n",
    "        for i in range(len(textFile)-1):\n",
    "            if textFile[i] + textFile[i+1] not in kGrams:\n",
    "                kGrams.add(textFile[i] + textFile[i+1])\n",
    "    return kGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. \"I am Sam\" = { ['I a'], [' am'], ['am '], ['m S'], [' Sa'], ['Sam']}\n",
    "def threeCharGram(dataset):\n",
    "    with open(dataset, 'r') as data:\n",
    "        textFile = data.read().replace('\\n', ' ')\n",
    "        kGrams = set()\n",
    "        # 3-Char gram\n",
    "        for i in range(len(textFile)-2):\n",
    "            if textFile[i] + textFile[i+1] + textFile[i+2] not in kGrams:\n",
    "                kGrams.add(textFile[i] + textFile[i+1] + textFile[i+2])\n",
    "    return kGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. \"I am Sam\" = { ['I am'], ['am Sam']}\n",
    "def twoWordGram(dataset):\n",
    "    with open(dataset, 'r') as data:\n",
    "        tokens = str.split(data.read().replace('\\n', ' '))\n",
    "        kGrams = set()\n",
    "        #2-word gram\n",
    "        for i in range(len(tokens)-1):\n",
    "            if tokens[i] + ' ' + tokens[i+1] not in kGrams:\n",
    "                kGrams.add(tokens[i] + ' ' + tokens[i+1])\n",
    "    return kGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1.txt two char gram size: 263\n",
      "D2.txt two char gram size: 262\n",
      "D3.txt two char gram size: 269\n",
      "D4.txt two char gram size: 255\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['D1.txt','D2.txt','D3.txt','D4.txt']:\n",
    "    print(dataset + ' two char gram size: %d' % len(twoCharGram('documents/'+dataset)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1.txt three char gram size: 765\n",
      "D2.txt three char gram size: 762\n",
      "D3.txt three char gram size: 828\n",
      "D4.txt three char gram size: 698\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['D1.txt','D2.txt','D3.txt','D4.txt']:\n",
    "    print(dataset + ' three char gram size: %d' % len(threeCharGram('documents/'+dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1.txt two word gram size: 279\n",
      "D2.txt two word gram size: 278\n",
      "D3.txt two word gram size: 337\n",
      "D4.txt two word gram size: 232\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['D1.txt','D2.txt','D3.txt','D4.txt']:\n",
    "    print(dataset + ' two word gram size: %d' % len(twoWordGram('documents/'+dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard Similarity is defined\n",
    "$JS(A,B)=$\n",
    "> # $\\frac{|A\\cap B|}{|A\\cap B|+ |A\\Delta B|} = \\frac{|A\\cap B|}{|A\\cup B|}$\n",
    "\n",
    "Consider two sets A = {0,1,2,5,6} and B = {0,2,3,5,7,9}. How similar are A and B? The Jaccard similarity is defined\n",
    "\n",
    "$JS(A, B) = \\frac{|A \\cap B|}{|A\\cup B|}$\n",
    "$= \\frac{|{0,2,5}|}{|{0,1,2,3,5,6,7,9}|} =  \\frac{3}{8} = 0.375$\n",
    "\n",
    "Given a set A, the cardinality of A denoted |A| counts how many elements are in A. The intersection between two sets A and B is denoted A ∩ B and reveals all items which are in both sets. The union between two sets A and B is denoted A ∪ B and reveals all items which are in either set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(d1,d2,d3,d4, message):\n",
    "    print('D1.txt\\'s '+ message +' similarity with D2.txt is: %.6f percent' % (100.* len(D1set.intersection(D2set))/ len(D1set.union(D2set))))\n",
    "    print('D1.txt\\'s '+ message +' similarity with D3.txt is: %.6f percent' % (100.* len(D1set.intersection(D3set))/ len(D1set.union(D3set))))\n",
    "    print('D1.txt\\'s '+ message +' similarity with D4.txt is: %.6f percent' % (100.* len(D1set.intersection(D4set))/ len(D1set.union(D4set))))\n",
    "    print('D2.txt\\'s '+ message +' similarity with D3.txt is: %.6f percent' % (100.* len(D2set.intersection(D3set))/ len(D2set.union(D3set))))\n",
    "    print('D2.txt\\'s '+ message +' similarity with D4.txt is: %.6f percent' % (100.* len(D2set.intersection(D4set))/ len(D2set.union(D4set))))\n",
    "    print('D3.txt\\'s '+ message +' similarity with D4.txt is: %.6f percent' % (100.* len(D3set.intersection(D4set))/ len(D3set.union(D4set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1.txt's two char gram similarity with D2.txt is: 98.113208 percent\n",
      "D1.txt's two char gram similarity with D3.txt is: 81.569966 percent\n",
      "D1.txt's two char gram similarity with D4.txt is: 64.444444 percent\n",
      "D2.txt's two char gram similarity with D3.txt is: 80.000000 percent\n",
      "D2.txt's two char gram similarity with D4.txt is: 64.126984 percent\n",
      "D3.txt's two char gram similarity with D4.txt is: 65.299685 percent\n"
     ]
    }
   ],
   "source": [
    "D1set = twoCharGram('documents/D1.txt')\n",
    "D2set = twoCharGram('documents/D2.txt')\n",
    "D3set = twoCharGram('documents/D3.txt')\n",
    "D4set = twoCharGram('documents/D4.txt')\n",
    "jaccard(D1set,D2set,D3set,D4set, 'two char gram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1.txt's two char gram similarity with D2.txt is: 97.797927 percent\n",
      "D1.txt's two char gram similarity with D3.txt is: 58.035714 percent\n",
      "D1.txt's two char gram similarity with D4.txt is: 30.508475 percent\n",
      "D2.txt's two char gram similarity with D3.txt is: 56.804734 percent\n",
      "D2.txt's two char gram similarity with D4.txt is: 30.590340 percent\n",
      "D3.txt's two char gram similarity with D4.txt is: 31.212382 percent\n"
     ]
    }
   ],
   "source": [
    "D1set = threeCharGram('documents/D1.txt')\n",
    "D2set = threeCharGram('documents/D2.txt')\n",
    "D3set = threeCharGram('documents/D3.txt')\n",
    "D4set = threeCharGram('documents/D4.txt')\n",
    "jaccard(D1set,D2set,D3set,D4set, 'two char gram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1.txt's two char gram similarity with D2.txt is: 94.076655 percent\n",
      "D1.txt's two char gram similarity with D3.txt is: 18.234165 percent\n",
      "D1.txt's two char gram similarity with D4.txt is: 3.024194 percent\n",
      "D2.txt's two char gram similarity with D3.txt is: 17.366412 percent\n",
      "D2.txt's two char gram similarity with D4.txt is: 3.030303 percent\n",
      "D3.txt's two char gram similarity with D4.txt is: 1.607143 percent\n"
     ]
    }
   ],
   "source": [
    "D1set = twoWordGram('documents/D1.txt')\n",
    "D2set = twoWordGram('documents/D2.txt')\n",
    "D3set = twoWordGram('documents/D3.txt')\n",
    "D4set = twoWordGram('documents/D4.txt')\n",
    "jaccard(D1set,D2set,D3set,D4set, 'two char gram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minhashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer to http://www.cs.utah.edu/~jeffp/DMBook/L4-Minhash for more information on Minhashing\n",
    "D1Gram = threeCharGram('documents/D1.txt')\n",
    "D2Gram = threeCharGram('documents/D2.txt')\n",
    "DTotal = list(D1Gram.union(D2Gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with t = 20  we get a minhash similarity of  0.95\n",
      "with t = 60  we get a minhash similarity of  0.9666666666666667\n",
      "with t = 150  we get a minhash similarity of  0.9866666666666667\n",
      "with t = 300  we get a minhash similarity of  0.9833333333333333\n",
      "with t = 600  we get a minhash similarity of  0.9683333333333334\n"
     ]
    }
   ],
   "source": [
    "# Implementing fast Minhashing algortihm\n",
    "for k in [20,60,150,300,600]:    \n",
    "    successCounter = 0\n",
    "    for t in range (k):\n",
    "        minNum = [math.inf, math.inf]\n",
    "        for i in range (len(DTotal)):\n",
    "            current = hash(str(t)+DTotal[i]+str(t)) % 10000\n",
    "            if DTotal[i] in D1Gram: # this is how we'll emulate the vector representation of this D1\n",
    "                if (current < minNum[0]):\n",
    "                    minNum[0] = current\n",
    "            if DTotal[i] in D2Gram: # this is how we'll emulate the vector representation of this D2\n",
    "                if (current < minNum[1]):\n",
    "                    minNum[1] = current\n",
    "        if minNum[0] == minNum[1]:\n",
    "            successCounter = successCounter+1\n",
    "    print(\"with t = %d\"%k, \" we get a minhash similarity of \", successCounter/k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
